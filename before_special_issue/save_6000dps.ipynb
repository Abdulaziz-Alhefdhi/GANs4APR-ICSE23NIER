{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3113it [00:00, 31123.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting file paths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21757it [00:00, 45638.42it/s]\n",
      "100%|██████████| 43512/43512 [00:00<00:00, 1660412.28it/s]\n",
      "100%|██████████| 43512/43512 [00:00<00:00, 1713494.22it/s]\n",
      " 17%|█▋        | 3778/21756 [00:00<00:00, 37778.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting file contents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21756/21756 [00:00<00:00, 37452.79it/s]\n",
      "100%|██████████| 21756/21756 [00:00<00:00, 42136.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed files:-\n",
      "21756 buggy files + 21756 fixed files =  43512 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_dir = \"/media/aziz/Data/Aziz/data/gans_for_apr/training/\"\n",
    "\n",
    "print(\"Collecting file paths...\")\n",
    "file_paths = [root+'/'+name for root, dirs, files in tqdm(os.walk(data_dir)) for name in files]\n",
    "buggy_paths = [f_path for f_path in tqdm(file_paths) if 'buggy' in f_path]\n",
    "fixed_paths = [f_path for f_path in tqdm(file_paths) if 'fixed' in f_path]\n",
    "\n",
    "print(\"Collecting file contents...\")\n",
    "buggy_data = []\n",
    "for path in tqdm(buggy_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        buggy_data.append(f.read())\n",
    "fixed_data = []\n",
    "for path in tqdm(fixed_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        fixed_data.append(f.read())\n",
    "\n",
    "print('Number of processed files:-')\n",
    "print(len(buggy_data), 'buggy files +', len(fixed_data), 'fixed files = ', len(buggy_data)+len(fixed_data), 'files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data - Reduce noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21756/21756 [00:00<00:00, 81611.64it/s]\n",
      "100%|██████████| 21756/21756 [00:00<00:00, 62513.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments have been removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_comments(code):\n",
    "    code = re.sub('(?s)/\\*.*?\\*/', '', code)\n",
    "    return re.sub('(//[^\\n]*)', '', code)\n",
    "\n",
    "\n",
    "buggy_no_comments = [remove_comments(x) for x in tqdm(buggy_data)]\n",
    "fixed_no_comments = [remove_comments(x) for x in tqdm(fixed_data)]\n",
    "print('Comments have been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21756/21756 [00:01<00:00, 16783.57it/s]\n",
      "100%|██████████| 21756/21756 [00:01<00:00, 16245.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White spaces have been removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_spaces(code):\n",
    "    '''This function removes excessive spaces and keeps necessary ones'''\n",
    "    code = code.splitlines()\n",
    "    result = []\n",
    "    for line in code:\n",
    "        line = line.split()\n",
    "        line = ' '.join(line)\n",
    "        if len(line) > 0:  # Remove empty lines\n",
    "            result.append(line)\n",
    "    \n",
    "    return '\\n'.join(result)\n",
    "\n",
    "\n",
    "buggy_no_spaces = [remove_spaces(x) for x in tqdm(buggy_no_comments)]\n",
    "fixed_no_spaces = [remove_spaces(x) for x in tqdm(fixed_no_comments)]\n",
    "print('White spaces have been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove identical buggy and fixed codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4726 code pairs have been removed\n",
      "Current # data points: 17030\n"
     ]
    }
   ],
   "source": [
    "buggy_texts, fixed_texts = [], []\n",
    "for buggy, fixed in zip(buggy_no_spaces, fixed_no_spaces):\n",
    "    if buggy != fixed:\n",
    "        buggy_texts.append(buggy)\n",
    "        fixed_texts.append(fixed)\n",
    "print(len(buggy_no_spaces)-len(buggy_texts), 'code pairs have been removed')\n",
    "print('Current # data points:', len(buggy_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated pairs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3857 code pairs have been removed\n",
      "Current # data points: 13173\n"
     ]
    }
   ],
   "source": [
    "text_pairs = [(x, y) for x, y in zip(buggy_texts, fixed_texts)]\n",
    "code_pairs = sorted(set(text_pairs))  # Sorted to ensure same order every run (not sure if 'set' works randomly)\n",
    "\n",
    "buggy_codes = [x[0] for x in code_pairs]\n",
    "fixed_codes = [x[1] for x in code_pairs]\n",
    "\n",
    "print(len(text_pairs)-len(code_pairs), 'code pairs have been removed')\n",
    "print('Current # data points:', len(buggy_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13173it [00:06, 2181.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "\n",
    "\n",
    "initial_diffs = [list(ndiff(x.splitlines(), y.splitlines())) for x, y in tqdm(zip(buggy_codes, fixed_codes))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to one-line difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13173/13173 [00:00<00:00, 40512.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 code pairs have been removed\n",
      "Current # data points: 12851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "one_line_diffs = []\n",
    "for diff in tqdm(initial_diffs):\n",
    "    if sum([1 for x in diff if x.startswith('-')]) > 1 or sum([1 for x in diff if x.startswith('+')]) > 1:\n",
    "        continue\n",
    "    one_line_diffs.append(diff)\n",
    "\n",
    "print(len(initial_diffs)-len(one_line_diffs), 'code pairs have been removed')\n",
    "print('Current # data points:', len(one_line_diffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to modified lines (exclude added/deleted lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12851/12851 [00:00<00:00, 59870.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7102 code pairs have been removed\n",
      "Current # data points: 5749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for diff in tqdm(one_line_diffs):\n",
    "    for i, x in enumerate(diff):\n",
    "        if x.startswith('+') and diff[i-1].startswith('-'):\n",
    "            diffs.append((diff[i-1][2:], x[2:]))\n",
    "\n",
    "print(len(one_line_diffs)-len(diffs), 'code pairs have been removed')\n",
    "print('Current # data points:', len(diffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and separate training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been writte to disk\n"
     ]
    }
   ],
   "source": [
    "from random import seed, sample\n",
    "import pickle\n",
    "\n",
    "\n",
    "seed(30)\n",
    "shuf_diffs = sample(diffs, k=len(diffs))\n",
    "test_diffs, train_diffs = shuf_diffs[:500], shuf_diffs[500:]\n",
    "\n",
    "train_buggy_lines = [x[0] for x in train_diffs]\n",
    "train_fixed_lines = [x[1] for x in train_diffs]\n",
    "\n",
    "test_buggy_lines = [x[0] for x in test_diffs]\n",
    "test_fixed_lines = [x[1] for x in test_diffs]\n",
    "\n",
    "\n",
    "with open('train_buggy_lines.pkl', 'wb') as f:\n",
    "    pickle.dump(train_buggy_lines, f)\n",
    "with open('train_fixed_lines.pkl', 'wb') as f:\n",
    "    pickle.dump(train_fixed_lines, f)\n",
    "with open('test_buggy_lines.pkl', 'wb') as f:\n",
    "    pickle.dump(test_buggy_lines, f)\n",
    "with open('test_fixed_lines.pkl', 'wb') as f:\n",
    "    pickle.dump(test_fixed_lines, f)\n",
    "\n",
    "print(\"Data have been writte to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
