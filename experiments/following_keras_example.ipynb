{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy codes:-\n",
      "['(', '(', 'x', ' ', '+', ' ', 'y', ')', ' ', '>', '=', ' ', '(', 'z', ' ', '-', ' ', '1', ')', ')']\n",
      "['(', 'a', ' ', '&', '&', ' ', 'b', ')']\n",
      "['(', 'c', ' ', '>', ' ', '0', ')']\n",
      "['d']\n",
      "['(', 'e', ' ', '>', ' ', 'f', ')']\n",
      "====================\n",
      "Fixed codes:-\n",
      "['<soc>', '(', '(', 'x', ' ', '+', ' ', 'y', ')', ' ', '>', ' ', '(', 'z', ' ', '-', ' ', '1', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'a', ' ', '&', '&', ' ', '!', '(', 'b', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'c', ' ', '>', ' ', '1', ')', '<eoc>']\n",
      "['<soc>', '!', '(', 'd', ')', '<eoc>']\n",
      "['<soc>', '(', 'f', ' ', '>', ' ', 'e', ')', '<eoc>']\n",
      "====================\n",
      "{'!': 1, '&': 2, '(': 3, ')': 4, '+': 5, '-': 6, '0': 7, '1': 8, '<eoc>': 9, '<soc>': 10, '=': 11, '>': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'x': 19, 'y': 20, 'z': 21, ' ': 0}\n",
      "=====\n",
      "{1: '!', 2: '&', 3: '(', 4: ')', 5: '+', 6: '-', 7: '0', 8: '1', 9: '<eoc>', 10: '<soc>', 11: '=', 12: '>', 13: 'a', 14: 'b', 15: 'c', 16: 'd', 17: 'e', 18: 'f', 19: 'x', 20: 'y', 21: 'z', 0: ' '}\n",
      "====================\n",
      "Number of data points: 5\n",
      "Vocabulary size: 22\n",
      "Max length in buggy codes: 20\n",
      "Max length in fixed codes: 21\n"
     ]
    }
   ],
   "source": [
    "buggy_data = ['((x + y) >= (z - 1))',\n",
    "              '(a && b)',\n",
    "              '(c > 0)',\n",
    "              'd',\n",
    "              '(e > f)']\n",
    "fixed_data = ['((x + y) > (z - 1))',\n",
    "              '(a && !(b))',\n",
    "              '(c > 1)',\n",
    "              '!(d)',\n",
    "              '(f > e)']\n",
    "\n",
    "buggy_codes = [list(x) for x in buggy_data]\n",
    "fixed_codes = [['<soc>']+list(x)+['<eoc>'] for x in fixed_data]\n",
    "\n",
    "print(\"Buggy codes:-\")\n",
    "for x in buggy_codes:\n",
    "    print(x)\n",
    "print(\"====================\")\n",
    "print(\"Fixed codes:-\")\n",
    "for x in fixed_codes:\n",
    "    print(x)\n",
    "print(\"====================\")\n",
    "\n",
    "vocab = set([x for y in buggy_codes for x in y]+[x for y in fixed_codes for x in y])\n",
    "token_int_map = dict([(token, i+1) for i, token in enumerate(sorted(vocab-{' '}))])\n",
    "token_int_map[' '] = 0\n",
    "int_token_map = dict((i, token) for token, i in token_int_map.items())\n",
    "\n",
    "print(token_int_map)\n",
    "print(\"=====\")\n",
    "print(int_token_map)\n",
    "print(\"====================\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "max_buggy_len = max([len(txt) for txt in buggy_codes])\n",
    "max_fixed_len = max([len(txt) for txt in fixed_codes])\n",
    "num_dps = len(fixed_codes)\n",
    "\n",
    "print('Number of data points:', num_dps)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print('Max length in buggy codes:', max_buggy_len)\n",
    "print('Max length in fixed codes:', max_fixed_len)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "buggy_inputs =  np.zeros((num_dps, max_buggy_len, vocab_size), dtype='float32')\n",
    "fixed_inputs =  np.zeros((num_dps, max_fixed_len, vocab_size), dtype='float32')\n",
    "fixed_outputs = np.zeros((num_dps, max_fixed_len, vocab_size), dtype='float32')\n",
    "\n",
    "buggy_inputs[:, :, 0] = 1.\n",
    "fixed_inputs[:, :, 0] = 1.\n",
    "fixed_outputs[:, :, 0] = 1.\n",
    "\n",
    "for i, (buggy, fixed) in enumerate(zip(buggy_codes, fixed_codes)):\n",
    "    for t, token in enumerate(buggy):\n",
    "        buggy_inputs[i, t, 0] = 0.\n",
    "        buggy_inputs[i, t, token_int_map[token]] = 1.\n",
    "    for t, token in enumerate(fixed):\n",
    "        int_value = token_int_map[token]\n",
    "        fixed_inputs[i, t, 0] = 0.\n",
    "        fixed_inputs[i, t, int_value] = 1.\n",
    "        if t > 0:\n",
    "            fixed_outputs[i, t-1, 0] = 0.\n",
    "            fixed_outputs[i, t-1, int_value] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 19:53:29.113529 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0918 19:53:29.124272 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0918 19:53:29.127001 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5\n",
      "Vocabulary size: 22\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 21\n",
      "====================\n",
      "Token-integer mapping:-\n",
      "{' ': 0, '!': 1, '&': 2, '(': 3, ')': 4, '+': 5, '-': 6, '0': 7, '1': 8, '<eoc>': 9, '<soc>': 10, '=': 11, '>': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'x': 19, 'y': 20, 'z': 21}\n",
      "{0: ' ', 1: '!', 2: '&', 3: '(', 4: ')', 5: '+', 6: '-', 7: '0', 8: '1', 9: '<eoc>', 10: '<soc>', 11: '=', 12: '>', 13: 'a', 14: 'b', 15: 'c', 16: 'd', 17: 'e', 18: 'f', 19: 'x', 20: 'y', 21: 'z'}\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0918 19:53:29.568046 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0918 19:53:29.583596 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0918 19:53:29.674618 140057484760832 deprecation.py:323] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0918 19:53:30.584087 140057484760832 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 2s 423ms/step - loss: 3.0823 - acc: 0.0095\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8565 - acc: 0.6667\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.2644 - acc: 0.6476\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7351 - acc: 0.6095\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5565 - acc: 0.6381\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.5498 - acc: 0.6667\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.5153 - acc: 0.6381\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.5262 - acc: 0.6476\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4215 - acc: 0.6381\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3997 - acc: 0.6667\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3787 - acc: 0.6381\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3684 - acc: 0.6667\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3649 - acc: 0.6476\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.4036 - acc: 0.6667\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3803 - acc: 0.6381\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3961 - acc: 0.6667\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.3267 - acc: 0.6381\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3078 - acc: 0.6667\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2856 - acc: 0.6667\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3181 - acc: 0.6667\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2754 - acc: 0.6667\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.2898 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2554 - acc: 0.6667\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.2855 - acc: 0.6667\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.2215 - acc: 0.6667\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.2086 - acc: 0.6667\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1531 - acc: 0.6762\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1490 - acc: 0.6667\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.7225 - acc: 0.6571\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3615 - acc: 0.6667\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2564 - acc: 0.6667\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1951 - acc: 0.6571\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1908 - acc: 0.6667\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1857 - acc: 0.6667\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1353 - acc: 0.6667\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1068 - acc: 0.6667\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.0798 - acc: 0.6667\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0605 - acc: 0.6667\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0606 - acc: 0.6667\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1315 - acc: 0.6667\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1516 - acc: 0.6762\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1229 - acc: 0.6667\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0261 - acc: 0.6667\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9872 - acc: 0.6667\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9730 - acc: 0.6667\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0092 - acc: 0.6762\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1549 - acc: 0.6857\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.0856 - acc: 0.7048\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9860 - acc: 0.6857\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9408 - acc: 0.6857\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0197 - acc: 0.6762\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0823 - acc: 0.6667\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8957 - acc: 0.7333\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8552 - acc: 0.6857\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8215 - acc: 0.7143\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8040 - acc: 0.6857\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.8040 - acc: 0.7238\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9306 - acc: 0.6952\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.1136 - acc: 0.6762\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.2308 - acc: 0.7048\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.8813 - acc: 0.7429\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8154 - acc: 0.6857\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7711 - acc: 0.7619\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7384 - acc: 0.7143\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7217 - acc: 0.7714\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7360 - acc: 0.7143\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8030 - acc: 0.7619\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9091 - acc: 0.7048\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7802 - acc: 0.7810\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7997 - acc: 0.7143\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.4317 - acc: 0.6000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8288 - acc: 0.7524\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7227 - acc: 0.7333\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6795 - acc: 0.7429\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6520 - acc: 0.7905\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6306 - acc: 0.7714\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6190 - acc: 0.8286\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6530 - acc: 0.7714\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8286 - acc: 0.7429\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9255 - acc: 0.7333\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6752 - acc: 0.8095\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6344 - acc: 0.7238\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6427 - acc: 0.8381\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5683 - acc: 0.7905\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5457 - acc: 0.8571\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5297 - acc: 0.8190\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5239 - acc: 0.8571\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5302 - acc: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5811 - acc: 0.8095\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6078 - acc: 0.8000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7131 - acc: 0.7524\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6147 - acc: 0.8190\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5525 - acc: 0.8095\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4764 - acc: 0.9143\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4528 - acc: 0.8667\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4428 - acc: 0.8857\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4630 - acc: 0.8381\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4937 - acc: 0.8476\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5995 - acc: 0.7524\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4749 - acc: 0.8952\n",
      "-\n",
      "Input sentence: ((x + y) >= (z - 1))\n",
      "Decoded sentence: ((x + y))             \n",
      "-\n",
      "Input sentence: (a && b)\n",
      "Decoded sentence: (a &&&!(b)))<eoc>\n",
      "-\n",
      "Input sentence: (c > 0)\n",
      "Decoded sentence: (f >  ))              \n",
      "-\n",
      "Input sentence: d\n",
      "Decoded sentence: (( >  )<eoc>\n",
      "-\n",
      "Input sentence: (e > f)\n",
      "Decoded sentence: (f >  ))              \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "buggy_data = ['((x + y) >= (z - 1))',\n",
    "              '(a && b)',\n",
    "              '(c > 0)',\n",
    "              'd',\n",
    "              '(e > f)']\n",
    "fixed_data = ['((x + y) > (z - 1))',\n",
    "              '(a && !(b))',\n",
    "              '(c > 1)',\n",
    "              '!(d)',\n",
    "              '(f > e)']\n",
    "\n",
    "\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Vectorize the data.\n",
    "buggy_codes = [list(x) for x in buggy_data]\n",
    "fixed_codes = [['<soc>']+list(x)+['<eoc>'] for x in fixed_data]\n",
    "vocab = set([x for y in buggy_codes for x in y] + [x for y in fixed_codes for x in y])\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "vocab_size = len(vocab)\n",
    "max_encoder_seq_length = max([len(x) for x in buggy_codes])\n",
    "max_decoder_seq_length = max([len(x) for x in fixed_codes])\n",
    "token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(vocab)])\n",
    "reverse_token_index = dict(\n",
    "    (i, char) for char, i in token_index.items())\n",
    "\n",
    "num_dps = len(buggy_codes)\n",
    "\n",
    "print('Number of samples:', num_dps)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print(\"====================\")\n",
    "print('Token-integer mapping:-')\n",
    "print(token_index)\n",
    "print(reverse_token_index)\n",
    "print(\"====================\")\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_dps, max_encoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_dps, max_decoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_dps, max_decoder_seq_length, vocab_size),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (buggy_code, fixed_code) in enumerate(zip(buggy_codes, fixed_codes)):\n",
    "    for t, char in enumerate(buggy_code):\n",
    "        encoder_input_data[i, t, token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, token_index[' ']] = 1.\n",
    "    for t, char in enumerate(fixed_code):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, token_index[' ']] = 1.\n",
    "\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, vocab_size))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, vocab_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          epochs=epochs)\n",
    "# Save model\n",
    "# model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocab_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, token_index['<soc>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_token_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<eoc>' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', buggy_data[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_fixed_ints(enc_dec, bugs, fixed_len, token_map, int_map):\n",
    "#     gntd_ints = np.zeros(shape=(len(bugs), fixed_len))\n",
    "#     gntd_ints[:, 0] = token_map[\"<soc>\"]\n",
    "#     for buggy, generated in zip(bugs, gntd_ints):\n",
    "#         buggy_input = buggy[np.newaxis]\n",
    "#         gntd_in_out = generated[np.newaxis]\n",
    "#         for i in range(1, fixed_len):\n",
    "#             prediction = enc_dec.predict([buggy_input, gntd_in_out]).argmax(axis=2)\n",
    "#             if int_map[prediction[:, i][0]] == \"<eoc>\":\n",
    "#                 break\n",
    "#             generated[i] = prediction[:, i]\n",
    "    \n",
    "#     return gntd_ints\n",
    "\n",
    "\n",
    "# def decode_ints(int_matrix, int_map):\n",
    "#     gntd_codes = []\n",
    "#     for ints in int_matrix:\n",
    "#         code = [int_map[x] for x in ints if x != 0]\n",
    "#         gntd_codes.append(code)\n",
    "        \n",
    "#     return gntd_codes\n",
    "\n",
    "\n",
    "# print('=============')\n",
    "# print('=============')\n",
    "# print('=============')\n",
    "# generated_ints = generate_fixed_ints(seq2seq, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "# generated_codes = decode_ints(generated_ints, int_token_map)\n",
    "# for buggy, fixed, gnrtd in zip(buggy_codes, fixed_codes, generated_codes):\n",
    "#     print('=============')\n",
    "#     print('Buggy code:', ' '.join(buggy[1:-1]))\n",
    "#     print('Fixed code:', ' '.join(fixed[1:-1]))\n",
    "#     print('Genration: ', ' '.join(gnrtd[1:]))\n",
    "\n",
    "\n",
    "\n",
    "def from_mats_to_seqs(mats, int_map):\n",
    "    gntd_ints = []\n",
    "    for matrix in mats:\n",
    "        gntd_seq = []\n",
    "        for row in matrix:\n",
    "            for i, token in enumerate(row):\n",
    "                if token == 1.:\n",
    "                    gntd_seq.append(i)\n",
    "                    if int_map[i] == \"<eoc>\":\n",
    "                        break\n",
    "        gntd_ints.append(gntd_seq)\n",
    "    \n",
    "    return gntd_ints\n",
    "\n",
    "\n",
    "def decode_ints(int_matrix, int_map):\n",
    "    gntd_codes = []\n",
    "    for ints in int_matrix:\n",
    "        code = [int_map[x] for x in ints if x != 0]\n",
    "        gntd_codes.append(code)\n",
    "        \n",
    "    return gntd_codes\n",
    "\n",
    "\n",
    "def generate_fixed_ints(enc_dec, bugs, fixed_len, v_size, token_map, int_map):\n",
    "    gntd_mats = np.zeros(shape=(len(bugs), fixed_len, v_size))\n",
    "    gntd_mats[:, 0, token_map[\"<soc>\"]] = 1.\n",
    "    gntd_mats[:, 1:, 0] = 1.\n",
    "    print(gntd_mats.shape)\n",
    "    for j, buggy in enumerate(bugs):  # for seq in dps\n",
    "        buggy_input = buggy[np.newaxis]\n",
    "        gntd_in_out = gntd_mats[j]\n",
    "        gntd_in_out = gntd_in_out[np.newaxis]\n",
    "        for i in range(1, fixed_len):  # for token in dp\n",
    "            prediction = enc_dec.predict([buggy_input, gntd_in_out]).argmax(axis=2)\n",
    "            gntd_mats[j, i, 0] = 0.\n",
    "            gntd_mats[j, i, prediction[:, i][0]] = 1.\n",
    "#             print(from_mats_to_seqs(gntd_mats, int_map))\n",
    "            if int_map[prediction[:, i][0]] == \"<eoc>\":\n",
    "                print('hi')\n",
    "                break\n",
    "#     gntd_ints = []\n",
    "#     for matrix in gntd_mats:\n",
    "#         gntd_seq = []\n",
    "#         for row in matrix:\n",
    "#             for i, token in enumerate(row):\n",
    "#                 if token == 1.:\n",
    "#                     if int_map[i] == \"<eoc>\":\n",
    "#                         break\n",
    "#                     gntd_seq.append(i)\n",
    "#             gntd_ints.append(gntd_seq)\n",
    "    \n",
    "    return from_mats_to_seqs(gntd_mats, int_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('=============')\n",
    "print('=============')\n",
    "print('=============')\n",
    "generated_ints = generate_fixed_ints(seq2seq, buggy_inputs, max_fixed_len, vocab_size, token_int_map, int_token_map)\n",
    "generated_codes = decode_ints(generated_ints, int_token_map)\n",
    "for buggy, fixed, gnrtd in zip(buggy_codes, fixed_codes, generated_codes):\n",
    "    print('=============')\n",
    "    print('Buggy code:', ' '.join(buggy))\n",
    "    print('Fixed code:', ' '.join(fixed))\n",
    "    print('Genration: ', ' '.join(gnrtd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<soc>',\n",
       "  '(',\n",
       "  '(',\n",
       "  'x',\n",
       "  '+',\n",
       "  'y',\n",
       "  ')',\n",
       "  '>',\n",
       "  '(',\n",
       "  'z',\n",
       "  '-',\n",
       "  '1',\n",
       "  ')',\n",
       "  ')',\n",
       "  '<eoc>'],\n",
       " ['<soc>', '(', 'a', '&', '&', '!', '(', 'b', ')', ')', '<eoc>'],\n",
       " ['<soc>', '(', 'c', '>', '1', ')', '<eoc>'],\n",
       " ['<soc>', '!', '(', 'd', ')', '<eoc>'],\n",
       " ['<soc>', '(', 'f', '>', 'e', ')', '<eoc>']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_ints(from_mats_to_seqs(fixed_inputs, int_token_map), int_token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['(', '(', 'x', '+', 'y', ')', '>', '(', 'z', '-', '1', ')', ')', '<eoc>'],\n",
       " ['(', 'a', '&', '&', '!', '(', 'b', ')', ')', '<eoc>'],\n",
       " ['(', 'c', '>', '1', ')', '<eoc>'],\n",
       " ['!', '(', 'd', ')', '<eoc>'],\n",
       " ['(', 'f', '>', 'e', ')', '<eoc>']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_ints(from_mats_to_seqs(fixed_outputs, int_token_map), int_token_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy codes:-\n",
      "['(', '(', 'x', '+', 'y', ')', '>', '=', '(', 'z', '-', '1', ')', ')']\n",
      "['(', 'a', '&', '&', 'b', ')']\n",
      "['(', 'c', '>', '0', ')']\n",
      "['d']\n",
      "['(', 'e', '>', 'f', ')']\n",
      "====================\n",
      "Fixed codes:-\n",
      "['<soc>', '(', '(', 'x', '+', 'y', ')', '>', '(', 'z', '-', '1', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'a', '&', '&', '!', '(', 'b', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'c', '>', '1', ')', '<eoc>']\n",
      "['<soc>', '!', '(', 'd', ')', '<eoc>']\n",
      "['<soc>', '(', 'f', '>', 'e', ')', '<eoc>']\n",
      "{1: 'f', 2: '+', 3: '>', 4: 'e', 5: 'z', 6: 'y', 7: '1', 8: 'x', 9: '=', 10: '<soc>', 11: 'b', 12: '(', 13: '&', 14: 'a', 15: '-', 16: 'd', 17: '0', 18: '<eoc>', 19: ')', 20: 'c', 21: '!', 0: '<pad/unknown>'}\n",
      "Number of data points: 5\n",
      "Vocabulary size: 22\n",
      "Max length in buggy codes: 14\n",
      "Max length in fixed codes: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 23:20:54.030289 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 23:20:54.040325 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 23:20:54.044531 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 23:20:54.357945 140003070756608 deprecation.py:323] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0913 23:20:54.389973 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0913 23:20:54.404683 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "W0913 23:20:56.466944 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 0.3442 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3658 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 309.7122 - model_1_loss: 0.6845 - model_2_loss: 3.0903\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2865 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4487 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 279.6088 - model_1_loss: 0.5293 - model_2_loss: 2.7908\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2657 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4313 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 420.9569 - model_1_loss: 0.6146 - model_2_loss: 4.2034\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2605 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3658 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 253.3992 - model_1_loss: 0.4162 - model_2_loss: 2.5298\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2534 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3450 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 231.6560 - model_1_loss: 0.4407 - model_2_loss: 2.3122\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2404 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3230 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 208.8307 - model_1_loss: 0.3507 - model_2_loss: 2.0848\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2231 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3033 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 183.8120 - model_1_loss: 0.4129 - model_2_loss: 1.8340\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2029 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2940 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 156.6920 - model_1_loss: 0.3168 - model_2_loss: 1.5638\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1823 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2944 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 162.2926 - model_1_loss: 0.4760 - model_2_loss: 1.6182\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1633 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2377 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 166.7368 - model_1_loss: 0.2242 - model_2_loss: 1.6651\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1472 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2033 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 143.8255 - model_1_loss: 0.5808 - model_2_loss: 1.4324\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1361 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1360 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 124.5954 - model_1_loss: 0.4534 - model_2_loss: 1.2414\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1218 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1277 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 113.6406 - model_1_loss: 0.2849 - model_2_loss: 1.1336\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0825 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 104.8861 - model_1_loss: 0.3269 - model_2_loss: 1.0456\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0455 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1583 - acc: 0.8000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 104.0241 - model_1_loss: 0.1152 - model_2_loss: 1.0391\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1404 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 85.5443 - model_1_loss: 0.4046 - model_2_loss: 0.8514\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0717 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 93.5957 - model_1_loss: 0.1489 - model_2_loss: 0.9345\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0272 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0991 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 81.5130 - model_1_loss: 0.2665 - model_2_loss: 0.8125\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1021 - acc: 0.8000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 63.5049 - model_1_loss: 2.4199 - model_2_loss: 0.6109\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0590 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 70.8213 - model_1_loss: 0.0969 - model_2_loss: 0.7072\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "Buggy code: ( x + y ) > = ( z - 1 )\n",
      "Fixed code: ( ( x + y ) > ( z - 1 ) )\n",
      "Genration:  ( ( ( x + y y ( y ( ) ) ( z\n",
      "=============\n",
      "Buggy code: a & & b\n",
      "Fixed code: ( a & & ! ( b ) )\n",
      "Genration:  ( a & & ! ( b ) )\n",
      "=============\n",
      "Buggy code: c > 0\n",
      "Fixed code: ( c > 1 )\n",
      "Genration:  ( c > ) )\n",
      "=============\n",
      "Buggy code: \n",
      "Fixed code: ! ( d )\n",
      "Genration:  ! ( d )\n",
      "=============\n",
      "Buggy code: e > f\n",
      "Fixed code: ( f > e )\n",
      "Genration:  ( f > ) )\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Concatenate, Embedding, LSTM, Dense, dot, Activation, concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.backend import argmax, cast\n",
    "\n",
    "\n",
    "def build_discriminator(dimension, v_size, buggy_len, fixed_len):\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    concatted = Concatenate()([buggy_input_layer, fixed_input_layer])\n",
    "    embed_lay = Embedding(v_size, dimension, mask_zero=True)(concatted)\n",
    "    x = LSTM(dimension)(embed_lay)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    disc = Model([buggy_input_layer, fixed_input_layer], out)\n",
    "    disc.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], loss_weights=[0.5])\n",
    "    \n",
    "    return disc\n",
    "\n",
    "\n",
    "def build_generator(dimension, v_size, buggy_len, fixed_len):\n",
    "    # Encoder\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    enc_embed_lay = Embedding(v_size, dimension, mask_zero=True)(buggy_input_layer)\n",
    "    encoder_outputs, state_h, state_c = LSTM(dimension, return_sequences=True, return_state=True)(enc_embed_lay)\n",
    "    # Decoder\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    dec_embed_lay = Embedding(v_size, dimension, mask_zero=True)(fixed_input_layer)\n",
    "    decoder_outputs = LSTM(dimension, return_sequences=True)(dec_embed_lay, initial_state=[state_h, state_c])\n",
    "    # Attention\n",
    "    attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])\n",
    "    attention = Activation('softmax', name='attention')(attention)\n",
    "    context = dot([attention, encoder_outputs], axes=[2, 1])\n",
    "    decoder_combined_context = concatenate([context, decoder_outputs])\n",
    "    attention_context_output = Dense(dimension, activation=\"tanh\")(decoder_combined_context)\n",
    "    # Model output\n",
    "    model_output = Dense(v_size, activation=\"softmax\")(attention_context_output)\n",
    "    # Build model\n",
    "    gen = Model([buggy_input_layer, fixed_input_layer], model_output)\n",
    "    \n",
    "    return gen\n",
    "\n",
    "\n",
    "def build_gan(gen, disc, buggy_len, fixed_len):\n",
    "    disc.trainable = False\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    gen_out = gen([buggy_input_layer, fixed_input_layer])\n",
    "    argmax_layer = Lambda(lambda x: cast(argmax(x, axis=2), dtype='float32'))\n",
    "    disc_out = disc([buggy_input_layer, argmax_layer(gen_out)])\n",
    "    gan = Model([buggy_input_layer, fixed_input_layer], [disc_out, gen_out])\n",
    "    # compile model\n",
    "    gan.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer='rmsprop', loss_weights=[1, 100])\n",
    "    \n",
    "    return gan\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "latent_dim = 512\n",
    "\n",
    "discriminator = build_discriminator(latent_dim, vocab_size, max_buggy_len, max_fixed_len)\n",
    "plot_model(discriminator, to_file='discriminator_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# Image('discriminator_model_plot.png')\n",
    "\n",
    "generator = build_generator(latent_dim, vocab_size, max_buggy_len, max_fixed_len)\n",
    "plot_model(generator, to_file='generator_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# Image('generator_model_plot.png')\n",
    "\n",
    "gan = build_gan(generator, discriminator, max_buggy_len, max_fixed_len)\n",
    "plot_model(gan, to_file='gan_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# gan.summary()\n",
    "# Image('gan_model_plot.png')\n",
    "\n",
    "\n",
    "def generate_fixed_ints(gen, bugs, fixed_len, token_map, int_map):\n",
    "    gntd_ints = np.zeros(shape=(len(bugs), fixed_len))\n",
    "    gntd_ints[:, 0] = token_map[\"<soc>\"]\n",
    "    for buggy, generated in zip(bugs, gntd_ints):\n",
    "        buggy_input = buggy[np.newaxis]\n",
    "        gntd_in_out = generated[np.newaxis]\n",
    "        for i in range(1, fixed_len):\n",
    "            prediction = gen.predict([buggy_input, gntd_in_out]).argmax(axis=2)\n",
    "            if int_map[prediction[:, i][0]] == \"<eoc>\":\n",
    "                break\n",
    "            generated[i] = prediction[:, i]\n",
    "    \n",
    "    return gntd_ints\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for e in range(epochs):\n",
    "    discriminator.fit([buggy_inputs, fixed_inputs], np.ones(num_dps))\n",
    "    generated_ints = generate_fixed_ints(generator, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "    discriminator.fit([buggy_inputs, generated_ints], np.zeros(num_dps))\n",
    "    gan.fit([buggy_inputs, fixed_inputs], [np.ones(num_dps), fixed_outputs])\n",
    "\n",
    "\n",
    "def decode_ints(int_matrix, int_map):\n",
    "    gntd_codes = []\n",
    "    for ints in int_matrix:\n",
    "        code = [int_map[x] for x in ints if x != 0]\n",
    "        gntd_codes.append(code)\n",
    "        \n",
    "    return gntd_codes\n",
    "\n",
    "\n",
    "print('=============')\n",
    "print('=============')\n",
    "print('=============')\n",
    "generated_ints = generate_fixed_ints(generator, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "generated_codes = decode_ints(generated_ints, int_token_map)\n",
    "for buggy, fixed, gnrtd in zip(buggy_codes, fixed_codes, generated_codes):\n",
    "    print('=============')\n",
    "    print('Buggy code:', ' '.join(buggy[1:-1]))\n",
    "    print('Fixed code:', ' '.join(fixed[1:-1]))\n",
    "    print('Genration: ', ' '.join(gnrtd[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
