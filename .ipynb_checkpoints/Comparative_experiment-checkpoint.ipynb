{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy_data = ['((x + y) >= (z - 1))',\n",
    "              '(a && b)',\n",
    "              '(c > 0)',\n",
    "              'd',\n",
    "              '(e > f)']\n",
    "fixed_data = ['((x + y) > (z - 1))',\n",
    "              '(a && !(b))',\n",
    "              '(c > 1)',\n",
    "              '!(d)',\n",
    "              '(f > e)']\n",
    "\n",
    "buggy_codes = [list(x.replace(' ', '')) for x in buggy_data]\n",
    "fixed_codes = [['<soc>']+list(x.replace(' ', ''))+['<eoc>'] for x in fixed_data]\n",
    "\n",
    "print(\"Buggy codes:-\")\n",
    "for x in buggy_codes:\n",
    "    print(x)\n",
    "print(\"====================\")\n",
    "print(\"Fixed codes:-\")\n",
    "for x in fixed_codes:\n",
    "    print(x)\n",
    "    \n",
    "vocab = set([x for y in buggy_codes for x in y]+[x for y in fixed_codes for x in y])\n",
    "token_int_map = dict([(token, i+1) for i, token in enumerate(vocab)])\n",
    "vocab.add('<pad/unknown>')\n",
    "token_int_map['<pad/unknown>'] = 0\n",
    "int_token_map = dict((i, token) for token, i in token_int_map.items())\n",
    "\n",
    "print(int_token_map)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "max_buggy_len = max([len(txt) for txt in buggy_codes])\n",
    "max_fixed_len = max([len(txt) for txt in fixed_codes])\n",
    "num_dps = len(fixed_codes)\n",
    "\n",
    "print('Number of data points:', num_dps)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print('Max length in buggy codes:', max_buggy_len)\n",
    "print('Max length in fixed codes:', max_fixed_len)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "buggy_inputs =  np.zeros((num_dps, max_buggy_len), dtype='int32')\n",
    "fixed_inputs =  np.zeros((num_dps, max_fixed_len), dtype='int32')\n",
    "fixed_outputs = np.zeros((num_dps, max_fixed_len, vocab_size), dtype='float32')\n",
    "\n",
    "for i, (buggy, fixed) in enumerate(zip(buggy_codes, fixed_codes)):\n",
    "    for t, token in enumerate(buggy):\n",
    "        buggy_inputs[i, t] = token_int_map[token]\n",
    "    for t, token in enumerate(fixed):\n",
    "        int_value = token_int_map[token]\n",
    "        fixed_inputs[i, t] = int_value\n",
    "        if t > 0:\n",
    "            fixed_outputs[i, t-1, int_value] = 1.\n",
    "    fixed_outputs[i, t, 0] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy codes:-\n",
      "['(', '(', 'x', '+', 'y', ')', '>', '=', '(', 'z', '-', '1', ')', ')']\n",
      "['(', 'a', '&', '&', 'b', ')']\n",
      "['(', 'c', '>', '0', ')']\n",
      "['d']\n",
      "['(', 'e', '>', 'f', ')']\n",
      "====================\n",
      "Fixed codes:-\n",
      "['<soc>', '(', '(', 'x', '+', 'y', ')', '>', '(', 'z', '-', '1', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'a', '&', '&', '!', '(', 'b', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'c', '>', '1', ')', '<eoc>']\n",
      "['<soc>', '!', '(', 'd', ')', '<eoc>']\n",
      "['<soc>', '(', 'f', '>', 'e', ')', '<eoc>']\n",
      "{1: '<soc>', 2: '=', 3: '!', 4: 'd', 5: '0', 6: 'b', 7: '-', 8: ')', 9: '>', 10: '+', 11: '<eoc>', 12: 'a', 13: '(', 14: 'x', 15: '1', 16: 'c', 17: 'e', 18: 'z', 19: 'y', 20: 'f', 21: '&', 0: '<pad/unknown>'}\n",
      "Number of data points: 5\n",
      "Vocabulary size: 22\n",
      "Max length in buggy codes: 14\n",
      "Max length in fixed codes: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 23:20:41.585980 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 23:20:41.596524 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 23:20:41.659328 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 23:20:41.972396 140029570557696 deprecation.py:323] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0913 23:20:42.325134 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0913 23:20:42.338889 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 14, 512)      11264       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 15, 512)      11264       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 14, 512), (N 2099200     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 15, 512)      2099200     embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 15, 14)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 15, 14)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 15, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 15, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 22)       11286       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,757,014\n",
      "Trainable params: 4,757,014\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 23:20:43.750517 140029570557696 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 2s 496ms/step - loss: 3.0900\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.7949\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3.8180\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.5059\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2151\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.0222\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.3447\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8509\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.6234\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.4560\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3355\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2754\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2872\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1902\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0768\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0027\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8407\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8303\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8441\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9054\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "Buggy code: ( x + y ) > = ( z - 1 )\n",
      "Fixed code: ( ( x + y ) > ( z - 1 ) )\n",
      "Genration:  ( x + y y ) )\n",
      "=============\n",
      "Buggy code: a & & b\n",
      "Fixed code: ( a & & ! ( b ) )\n",
      "Genration:  ( a & & ! ( b ) )\n",
      "=============\n",
      "Buggy code: c > 0\n",
      "Fixed code: ( c > 1 )\n",
      "Genration:  ( c > > )\n",
      "=============\n",
      "Buggy code: \n",
      "Fixed code: ! ( d )\n",
      "Genration:  ! ( d )\n",
      "=============\n",
      "Buggy code: e > f\n",
      "Fixed code: ( f > e )\n",
      "Genration:  ( f > > )\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, dot, Activation, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def build_lstm_encoder_decoder(dimension, v_size, buggy_len, fixed_len):\n",
    "    # Encoder\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    enc_embed_lay = Embedding(v_size, dimension,  mask_zero=True)(buggy_input_layer)\n",
    "    encoder_outputs, state_h, state_c = LSTM(dimension, return_sequences=True, return_state=True)(enc_embed_lay)\n",
    "    # Decoder\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    dec_embed_lay = Embedding(v_size, dimension, mask_zero=True)(fixed_input_layer)\n",
    "    decoder_outputs = LSTM(dimension, return_sequences=True)(dec_embed_lay, initial_state=[state_h, state_c])\n",
    "    # Attention\n",
    "    attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])\n",
    "    attention = Activation('softmax', name='attention')(attention)\n",
    "    context = dot([attention, encoder_outputs], axes=[2, 1])\n",
    "    decoder_combined_context = concatenate([context, decoder_outputs])\n",
    "    attention_context_output = Dense(dimension, activation=\"tanh\")(decoder_combined_context)\n",
    "    # Model output\n",
    "    model_output = Dense(v_size, activation=\"softmax\")(attention_context_output)\n",
    "    # Build & compile model\n",
    "    enc_dec = Model([buggy_input_layer, fixed_input_layer], model_output)\n",
    "    enc_dec.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    \n",
    "    return enc_dec\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "latent_dim = 512\n",
    "\n",
    "encoder_decoder = build_lstm_encoder_decoder(latent_dim, vocab_size, max_buggy_len, max_fixed_len)\n",
    "plot_model(encoder_decoder, to_file='lstm_encoder_decoder.png', show_shapes=True, show_layer_names=True)\n",
    "encoder_decoder.summary()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "encoder_decoder.fit([buggy_inputs, fixed_inputs], fixed_outputs, epochs=epochs)\n",
    "\n",
    "\n",
    "def generate_fixed_ints(enc_dec, bugs, fixed_len, token_map, int_map):\n",
    "    gntd_ints = np.zeros(shape=(len(bugs), fixed_len))\n",
    "    gntd_ints[:, 0] = token_map[\"<soc>\"]\n",
    "    for buggy, generated in zip(bugs, gntd_ints):\n",
    "        buggy_input = buggy[np.newaxis]\n",
    "        gntd_in_out = generated[np.newaxis]\n",
    "        for i in range(1, fixed_len):\n",
    "            prediction = enc_dec.predict([buggy_input, gntd_in_out]).argmax(axis=2)\n",
    "            if int_map[prediction[:, i][0]] == \"<eoc>\":\n",
    "                break\n",
    "            generated[i] = prediction[:, i]\n",
    "    \n",
    "    return gntd_ints\n",
    "\n",
    "\n",
    "def decode_ints(int_matrix, int_map):\n",
    "    gntd_codes = []\n",
    "    for ints in int_matrix:\n",
    "        code = [int_map[x] for x in ints if x != 0]\n",
    "        gntd_codes.append(code)\n",
    "        \n",
    "    return gntd_codes\n",
    "\n",
    "\n",
    "print('=============')\n",
    "print('=============')\n",
    "print('=============')\n",
    "generated_ints = generate_fixed_ints(encoder_decoder, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "generated_codes = decode_ints(generated_ints, int_token_map)\n",
    "for buggy, fixed, gnrtd in zip(buggy_codes, fixed_codes, generated_codes):\n",
    "    print('=============')\n",
    "    print('Buggy code:', ' '.join(buggy[1:-1]))\n",
    "    print('Fixed code:', ' '.join(fixed[1:-1]))\n",
    "    print('Genration: ', ' '.join(gnrtd[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy codes:-\n",
      "['(', '(', 'x', '+', 'y', ')', '>', '=', '(', 'z', '-', '1', ')', ')']\n",
      "['(', 'a', '&', '&', 'b', ')']\n",
      "['(', 'c', '>', '0', ')']\n",
      "['d']\n",
      "['(', 'e', '>', 'f', ')']\n",
      "====================\n",
      "Fixed codes:-\n",
      "['<soc>', '(', '(', 'x', '+', 'y', ')', '>', '(', 'z', '-', '1', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'a', '&', '&', '!', '(', 'b', ')', ')', '<eoc>']\n",
      "['<soc>', '(', 'c', '>', '1', ')', '<eoc>']\n",
      "['<soc>', '!', '(', 'd', ')', '<eoc>']\n",
      "['<soc>', '(', 'f', '>', 'e', ')', '<eoc>']\n",
      "{1: 'f', 2: '+', 3: '>', 4: 'e', 5: 'z', 6: 'y', 7: '1', 8: 'x', 9: '=', 10: '<soc>', 11: 'b', 12: '(', 13: '&', 14: 'a', 15: '-', 16: 'd', 17: '0', 18: '<eoc>', 19: ')', 20: 'c', 21: '!', 0: '<pad/unknown>'}\n",
      "Number of data points: 5\n",
      "Vocabulary size: 22\n",
      "Max length in buggy codes: 14\n",
      "Max length in fixed codes: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 23:20:54.030289 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 23:20:54.040325 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 23:20:54.044531 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 23:20:54.357945 140003070756608 deprecation.py:323] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0913 23:20:54.389973 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0913 23:20:54.404683 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "W0913 23:20:56.466944 140003070756608 deprecation_wrapper.py:119] From /home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 0.3442 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3658 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 309.7122 - model_1_loss: 0.6845 - model_2_loss: 3.0903\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2865 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aziz/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4487 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 279.6088 - model_1_loss: 0.5293 - model_2_loss: 2.7908\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2657 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4313 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 420.9569 - model_1_loss: 0.6146 - model_2_loss: 4.2034\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2605 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3658 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 253.3992 - model_1_loss: 0.4162 - model_2_loss: 2.5298\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2534 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3450 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 231.6560 - model_1_loss: 0.4407 - model_2_loss: 2.3122\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2404 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3230 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 208.8307 - model_1_loss: 0.3507 - model_2_loss: 2.0848\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2231 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3033 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 183.8120 - model_1_loss: 0.4129 - model_2_loss: 1.8340\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2029 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2940 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 156.6920 - model_1_loss: 0.3168 - model_2_loss: 1.5638\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1823 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2944 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 162.2926 - model_1_loss: 0.4760 - model_2_loss: 1.6182\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1633 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2377 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 166.7368 - model_1_loss: 0.2242 - model_2_loss: 1.6651\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1472 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2033 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 143.8255 - model_1_loss: 0.5808 - model_2_loss: 1.4324\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1361 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1360 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 124.5954 - model_1_loss: 0.4534 - model_2_loss: 1.2414\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1218 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1277 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 113.6406 - model_1_loss: 0.2849 - model_2_loss: 1.1336\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0825 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 104.8861 - model_1_loss: 0.3269 - model_2_loss: 1.0456\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0455 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1583 - acc: 0.8000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 104.0241 - model_1_loss: 0.1152 - model_2_loss: 1.0391\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1404 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 85.5443 - model_1_loss: 0.4046 - model_2_loss: 0.8514\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0717 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 93.5957 - model_1_loss: 0.1489 - model_2_loss: 0.9345\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0272 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0991 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 81.5130 - model_1_loss: 0.2665 - model_2_loss: 0.8125\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1021 - acc: 0.8000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 63.5049 - model_1_loss: 2.4199 - model_2_loss: 0.6109\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0590 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 70.8213 - model_1_loss: 0.0969 - model_2_loss: 0.7072\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "=============\n",
      "Buggy code: ( x + y ) > = ( z - 1 )\n",
      "Fixed code: ( ( x + y ) > ( z - 1 ) )\n",
      "Genration:  ( ( ( x + y y ( y ( ) ) ( z\n",
      "=============\n",
      "Buggy code: a & & b\n",
      "Fixed code: ( a & & ! ( b ) )\n",
      "Genration:  ( a & & ! ( b ) )\n",
      "=============\n",
      "Buggy code: c > 0\n",
      "Fixed code: ( c > 1 )\n",
      "Genration:  ( c > ) )\n",
      "=============\n",
      "Buggy code: \n",
      "Fixed code: ! ( d )\n",
      "Genration:  ! ( d )\n",
      "=============\n",
      "Buggy code: e > f\n",
      "Fixed code: ( f > e )\n",
      "Genration:  ( f > ) )\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Concatenate, Embedding, LSTM, Dense, dot, Activation, concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.backend import argmax, cast\n",
    "\n",
    "\n",
    "def build_discriminator(dimension, v_size, buggy_len, fixed_len):\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    concatted = Concatenate()([buggy_input_layer, fixed_input_layer])\n",
    "    embed_lay = Embedding(v_size, dimension, mask_zero=True)(concatted)\n",
    "    x = LSTM(dimension)(embed_lay)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    disc = Model([buggy_input_layer, fixed_input_layer], out)\n",
    "    disc.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], loss_weights=[0.5])\n",
    "    \n",
    "    return disc\n",
    "\n",
    "\n",
    "def build_generator(dimension, v_size, buggy_len, fixed_len):\n",
    "    # Encoder\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    enc_embed_lay = Embedding(v_size, dimension, mask_zero=True)(buggy_input_layer)\n",
    "    encoder_outputs, state_h, state_c = LSTM(dimension, return_sequences=True, return_state=True)(enc_embed_lay)\n",
    "    # Decoder\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    dec_embed_lay = Embedding(v_size, dimension, mask_zero=True)(fixed_input_layer)\n",
    "    decoder_outputs = LSTM(dimension, return_sequences=True)(dec_embed_lay, initial_state=[state_h, state_c])\n",
    "    # Attention\n",
    "    attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])\n",
    "    attention = Activation('softmax', name='attention')(attention)\n",
    "    context = dot([attention, encoder_outputs], axes=[2, 1])\n",
    "    decoder_combined_context = concatenate([context, decoder_outputs])\n",
    "    attention_context_output = Dense(dimension, activation=\"tanh\")(decoder_combined_context)\n",
    "    # Model output\n",
    "    model_output = Dense(v_size, activation=\"softmax\")(attention_context_output)\n",
    "    # Build model\n",
    "    gen = Model([buggy_input_layer, fixed_input_layer], model_output)\n",
    "    \n",
    "    return gen\n",
    "\n",
    "\n",
    "def build_gan(gen, disc, buggy_len, fixed_len):\n",
    "    disc.trainable = False\n",
    "    buggy_input_layer = Input(shape=(buggy_len,))\n",
    "    fixed_input_layer = Input(shape=(fixed_len,))\n",
    "    gen_out = gen([buggy_input_layer, fixed_input_layer])\n",
    "    argmax_layer = Lambda(lambda x: cast(argmax(x, axis=2), dtype='float32'))\n",
    "    disc_out = disc([buggy_input_layer, argmax_layer(gen_out)])\n",
    "    gan = Model([buggy_input_layer, fixed_input_layer], [disc_out, gen_out])\n",
    "    # compile model\n",
    "    gan.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer='rmsprop', loss_weights=[1, 100])\n",
    "    \n",
    "    return gan\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "latent_dim = 512\n",
    "\n",
    "discriminator = build_discriminator(latent_dim, vocab_size, max_buggy_len, max_fixed_len)\n",
    "plot_model(discriminator, to_file='discriminator_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# Image('discriminator_model_plot.png')\n",
    "\n",
    "generator = build_generator(latent_dim, vocab_size, max_buggy_len, max_fixed_len)\n",
    "plot_model(generator, to_file='generator_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# Image('generator_model_plot.png')\n",
    "\n",
    "gan = build_gan(generator, discriminator, max_buggy_len, max_fixed_len)\n",
    "plot_model(gan, to_file='gan_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# gan.summary()\n",
    "# Image('gan_model_plot.png')\n",
    "\n",
    "\n",
    "def generate_fixed_ints(gen, bugs, fixed_len, token_map, int_map):\n",
    "    gntd_ints = np.zeros(shape=(len(bugs), fixed_len))\n",
    "    gntd_ints[:, 0] = token_map[\"<soc>\"]\n",
    "    for buggy, generated in zip(bugs, gntd_ints):\n",
    "        buggy_input = buggy[np.newaxis]\n",
    "        gntd_in_out = generated[np.newaxis]\n",
    "        for i in range(1, fixed_len):\n",
    "            prediction = gen.predict([buggy_input, gntd_in_out]).argmax(axis=2)\n",
    "            if int_map[prediction[:, i][0]] == \"<eoc>\":\n",
    "                break\n",
    "            generated[i] = prediction[:, i]\n",
    "    \n",
    "    return gntd_ints\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for e in range(epochs):\n",
    "    discriminator.fit([buggy_inputs, fixed_inputs], np.ones(num_dps))\n",
    "    generated_ints = generate_fixed_ints(generator, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "    discriminator.fit([buggy_inputs, generated_ints], np.zeros(num_dps))\n",
    "    gan.fit([buggy_inputs, fixed_inputs], [np.ones(num_dps), fixed_outputs])\n",
    "\n",
    "\n",
    "def decode_ints(int_matrix, int_map):\n",
    "    gntd_codes = []\n",
    "    for ints in int_matrix:\n",
    "        code = [int_map[x] for x in ints if x != 0]\n",
    "        gntd_codes.append(code)\n",
    "        \n",
    "    return gntd_codes\n",
    "\n",
    "\n",
    "print('=============')\n",
    "print('=============')\n",
    "print('=============')\n",
    "generated_ints = generate_fixed_ints(generator, buggy_inputs, max_fixed_len, token_int_map, int_token_map)\n",
    "generated_codes = decode_ints(generated_ints, int_token_map)\n",
    "for buggy, fixed, gnrtd in zip(buggy_codes, fixed_codes, generated_codes):\n",
    "    print('=============')\n",
    "    print('Buggy code:', ' '.join(buggy[1:-1]))\n",
    "    print('Fixed code:', ' '.join(fixed[1:-1]))\n",
    "    print('Genration: ', ' '.join(gnrtd[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
