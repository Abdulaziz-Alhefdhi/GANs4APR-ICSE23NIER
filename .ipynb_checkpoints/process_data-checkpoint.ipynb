{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_dir = \"/media/aziz/Data/Aziz/data/gans_for_apr/\"\n",
    "\n",
    "file_paths = [root+'/'+name for root, dirs, files in os.walk(data_dir) for name in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20826/20826 [00:00<00:00, 60537.45it/s]\n",
      "100%|██████████| 20826/20826 [00:00<00:00, 59962.68it/s]\n"
     ]
    }
   ],
   "source": [
    "buggy_paths, fixed_paths = [], []\n",
    "buggy_paths = [f_path for f_path in file_paths if 'buggy' in f_path]\n",
    "fixed_paths = [f_path for f_path in file_paths if 'fixed' in f_path]\n",
    "\n",
    "buggy_texts = []\n",
    "for path in tqdm(buggy_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        buggy_texts.append(f.read())\n",
    "\n",
    "fixed_texts = []\n",
    "for path in tqdm(fixed_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        fixed_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " public void resetUid() {\n",
      "        uidOnlySchema = null;\n",
      "    }\n",
      "\n",
      "=======================\n",
      " public void resetUid() {\n",
      "        uidOnlySchema = null;\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(buggy_texts[1000])\n",
    "print('=======================')\n",
    "print(fixed_texts[1000])\n",
    "\n",
    "meld\n",
    "git diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20826it [00:00, 57207.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "buggy_codes, fixed_codes = [], []\n",
    "for buggy, fixed in tqdm(zip(buggy_texts, fixed_texts)):\n",
    "    buggy = re.sub('(?s)/\\*.*?\\*/', '', buggy)\n",
    "    buggy = re.sub('(//[^\\n]*)', '', buggy)\n",
    "    buggy_codes.append(buggy)\n",
    "    fixed = re.sub('(?s)/\\*.*?\\*/', '', fixed)\n",
    "    fixed = re.sub('(//[^\\n]*)', '', fixed)\n",
    "    fixed_codes.append(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_java_parser_ready(codes, output_dir, file_name):\n",
    "    with open(output_dir+file_name+'.java', \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"/**\\n * Dummy JavaDoc\\n */\\npublic class \"+file_name+\" {\\n\\n\")\n",
    "        for code in tqdm(codes):\n",
    "            f.write(\"/**\\n * Dummy JavaDoc\\n */\\n\"+str(code)+ \"\\n\")\n",
    "        f.write(\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18501.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23198.58it/s]\n"
     ]
    }
   ],
   "source": [
    "write_java_parser_ready(buggy_codes[:10], 'buggy_codes/', 'BuggyCodes')\n",
    "write_java_parser_ready(fixed_codes[:10], 'fixed_codes/', 'FixedCodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buggy_codes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def extract_codes_and_comments(data):\n",
    "    # Deal with codes\n",
    "    print(\"Extracting comments and code fragments...\")\n",
    "    codes, comments = [], []\n",
    "    for i in range(len(data)):\n",
    "        if i % 2 == 0:  # If a comment\n",
    "            if len(data[i]) == 1:  # If not a multiple comment\n",
    "                comments.append(\"\".join(data[i][0]))\n",
    "                codes.append(\" \".join(data[i+1]))\n",
    "    print(\"Ignoring defected code's pairs...\")\n",
    "    clean_codes, coms_v1 = [], []\n",
    "    for code, comment in zip(codes, comments):\n",
    "        flag = False\n",
    "        tokens = code.split()\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == '<' and tokens[i+1] == '<':\n",
    "                flag = True\n",
    "                break\n",
    "            if token == '>' and tokens[i+1] == '>':\n",
    "                flag = True\n",
    "                break\n",
    "            if token == 'Enumeration' and tokens[i+1] == 'enum':\n",
    "                flag = True\n",
    "                break\n",
    "            if token == '\\\\' and tokens[i+1] == b'\\xef\\xbf\\xbd'.decode(\"utf-8\", \"strict\"):\n",
    "                flag = True\n",
    "                break\n",
    "        if not flag:\n",
    "            clean_codes.append(code)\n",
    "            coms_v1.append(comment)\n",
    "\n",
    "    # Deal with comments\n",
    "    print(\"Removing whitespaces...\")\n",
    "    coms_v2 = []\n",
    "    for comment in coms_v1:\n",
    "        coms_v2.append(\" \".join(re.findall(r'\\S+|\\n', comment)).replace(' \\n ', '\\n'))\n",
    "    print(\"Removing commenting characters...\")\n",
    "    coms_multiline = []\n",
    "    for comment in coms_v2:\n",
    "        temp_list = comment.split(\"\\n\")  # Every line in a separate item of the list\n",
    "        temp_list2 = []\n",
    "        for item in temp_list:\n",
    "            stripped = item.lstrip(\"//\")\n",
    "            stripped = stripped.lstrip(\"/**\")\n",
    "            stripped = stripped.lstrip(\"/*\")\n",
    "            stripped = stripped.rstrip(\"*/\")\n",
    "            stripped = stripped.lstrip(\"*\")\n",
    "            temp_list2.append(stripped + \"\\n\")  # Append clean line to the new list\n",
    "        coms_multiline.append(\" \".join(temp_list2))\n",
    "    print(\"Making every comment a one-liner...\")\n",
    "    clean_coms = []\n",
    "    for comment in coms_multiline:\n",
    "        clean_coms.append(\" \".join(comment.split()))\n",
    "\n",
    "    return clean_codes, clean_coms, coms_multiline\n",
    "\n",
    "\n",
    "def write_to_file(path, file_name, a_list):\n",
    "    with open(path + file_name, \"w\", encoding='utf-8') as file:\n",
    "        for item in a_list:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "\n",
    "data_path = \"/home/aa043/sea/data/td/v3/\"\n",
    "\n",
    "# Retrieve data from disk\n",
    "print(\"Extracting data...\")\n",
    "with open(data_path+'pos.json', 'r') as f:\n",
    "    tds = json.load(f)\n",
    "print(len(tds)//2, \"TD observations extracted\")\n",
    "with open(data_path+'neg.json', 'r') as f:\n",
    "    non_tds = json.load(f)\n",
    "print(len(non_tds)//2, \"non-TD observations extracted\")\n",
    "\n",
    "# Prepare 1st draft of data\n",
    "print(\"Processing TD data...\")\n",
    "pos_clean_codes, pos_clean_coms, pos_coms_multiline = extract_codes_and_comments(tds)\n",
    "print(\"Processing non-TD data...\")\n",
    "neg_clean_codes, neg_clean_coms, neg_coms_multiline = extract_codes_and_comments(non_tds)\n",
    "\n",
    "print(\"Creating labels...\")\n",
    "pos_lbls = []\n",
    "for i in range(len(pos_clean_codes)):\n",
    "    pos_lbls.append(1)\n",
    "neg_lbls = []\n",
    "for i in range(len(neg_clean_codes)):\n",
    "    neg_lbls.append(0)\n",
    "\n",
    "print(\"Aggregating data...\")\n",
    "all_clean_codes = pos_clean_codes + neg_clean_codes\n",
    "all_clean_coms = pos_clean_coms + neg_clean_coms\n",
    "all_coms_multiline = pos_coms_multiline + neg_coms_multiline\n",
    "all_lbls = pos_lbls + neg_lbls\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "# Prepare for writing multiline comments to disk\n",
    "to_write_multi = []\n",
    "for comment in all_coms_multiline:\n",
    "    to_write_multi.append(comment + '+++')\n",
    "# Creating multiple files for JavaParser because of memory (heap) issues\n",
    "sliced_codes = [all_clean_codes[i:i+100000] for i in range(0, len(all_clean_codes), 100000)]\n",
    "\n",
    "# Writing to disk\n",
    "print(\"Writing the codes file...\")\n",
    "write_to_file(data_path, \"codes.txt\", all_clean_codes)\n",
    "print(\"Writing the comments file...\")\n",
    "write_to_file(data_path, \"comments.txt\", all_clean_coms)\n",
    "print(\"Writing the multiline comments file...\")\n",
    "write_to_file(data_path, \"comments_multiline.txt\", to_write_multi)\n",
    "print(\"Writing the labels file...\")\n",
    "write_to_file(data_path, \"labels.txt\", all_lbls)\n",
    "print(\"Writing the code file for JavaParser...\")\n",
    "method_count = 1\n",
    "for i, slice in enumerate(sliced_codes, 1):\n",
    "    with open(data_path+\"parser_processing/\"+str(i)+\"/Codes\"+str(i)+\".java\", \"w\", encoding='utf-8') as f:  # write codes file\n",
    "        f.write(\"/**\\n * Dummy JavaDoc\\n */\\npublic class Codes\"+str(i)+\" {\\n\\n\")\n",
    "        for code in slice:\n",
    "            f.write(\"/**\\n * Dummy JavaDoc\\n */\\npublic void coverMethod\" + str(method_count) + \"() {\\n\\t\" + str(code) + \"\\n}\\n\\n\")\n",
    "            method_count += 1\n",
    "        f.write(\"}\\n\")\n",
    "    print(\".java file\", i, \"has been written to disk\")\n",
    "print(str(method_count - 1) + \" conditional statements have been written to file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Aziz Aziz Aziz Aziz Aziz',\n",
       " '- Ibra Ibra Ibra Ibra Ibra',\n",
       " '?   ^    ^    ^    ^    ^\\n',\n",
       " '+ Ibla Ibla Ibla Ibla Ibla',\n",
       " '?   ^    ^    ^    ^    ^\\n',\n",
       " '  Ahmed Ahmed Ahmed Ahmed Ahmed']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import Differ\n",
    "\n",
    "\n",
    "text1 = \"Aziz Aziz Aziz Aziz Aziz\\nIbra Ibra Ibra Ibra Ibra\\nAhmed Ahmed Ahmed Ahmed Ahmed\".splitlines()\n",
    "text2 = \"Aziz Aziz Aziz Aziz Aziz\\nIbla Ibla Ibla Ibla Ibla\\nAhmed Ahmed Ahmed Ahmed Ahmed\".splitlines()\n",
    "text3 = '''Aziz Aziz Aziz Aziz Aziz\n",
    "Ibra Ibra Ibra Ibra Ibra\n",
    "Ahmed Ahmed Ahmed Ahmed Ahmed\n",
    "'''.splitlines(keepends=True)\n",
    "\n",
    "d = Differ()\n",
    "\n",
    "result = list(d.compare(text1, text2))\n",
    "# result = d.compare(text1, text2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  111111111111',\n",
       " '  2222222222222222',\n",
       " '- 333333333333',\n",
       " '  4444444444444444',\n",
       " '+ 5555555555555555',\n",
       " '  666666666',\n",
       " '  7777777777',\n",
       " '- abcdefghi',\n",
       " '+ 888888888888',\n",
       " '  99999999999']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "\n",
    "text4 = '''111111111111\n",
    "2222222222222222\n",
    "333333333333\n",
    "4444444444444444\n",
    "666666666\n",
    "7777777777\n",
    "abcdefghi\n",
    "99999999999\n",
    "'''.splitlines()\n",
    "\n",
    "\n",
    "text5 = '''111111111111\n",
    "2222222222222222\n",
    "4444444444444444\n",
    "5555555555555555\n",
    "666666666\n",
    "7777777777\n",
    "888888888888\n",
    "99999999999\n",
    "'''.splitlines()\n",
    "\n",
    "result2 = list(ndiff(text4, text5))\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result == list(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Aziz Aziz Aziz Aziz Aziz', '- Ibra Ibra Ibra Ibra Ibra', '?   ^    ^    ^    ^    ^\\n', '+ Ibla Ibla Ibla Ibla Ibla', '?   ^    ^    ^    ^    ^\\n', '  Ahmed Ahmed Ahmed Ahmed Ahmed']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Aziz Aziz Aziz Aziz Aziz', '- Ibra Ibra Ibra Ibra Ibra', '?   ^    ^    ^    ^    ^\\n', '+ Ibla Ibla Ibla Ibla Ibla', '?   ^    ^    ^    ^    ^\\n', '  Ahmed Ahmed Ahmed Ahmed Ahmed']\n"
     ]
    }
   ],
   "source": [
    "print(list(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
